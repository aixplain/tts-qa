{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "import editdistance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyannote.audio import Model\n",
    "from pyannote.audio.pipelines import VoiceActivityDetection\n",
    "from pydub import AudioSegment\n",
    "from tqdm import tqdm\n",
    "from whisper_model import WhisperASR\n",
    "\n",
    "import os\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "# read environment variables from vars.env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../vars.env\")\n",
    "\n",
    "# connect to postgresql db on localhost, post 5432, using user and password from vars.env\n",
    "\n",
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# Define the database credentials\n",
    "db_host = os.getenv(\"POSTGRES_HOST\")\n",
    "db_name = os.getenv(\"POSTGRES_DB\")\n",
    "db_user = os.getenv(\"POSTGRES_USER\")\n",
    "db_password = os.getenv(\"POSTGRES_PWD\")\n",
    "\n",
    "\n",
    "\n",
    "def edit_distance(s1, s2):\n",
    "    return editdistance.eval(s1, s2)\n",
    "\n",
    "\n",
    "# Connect to the database\n",
    "conn = psycopg2.connect(\n",
    "    host=db_host,\n",
    "    database=db_name,\n",
    "    user=db_user,\n",
    "    password=db_password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"Spanish\"\n",
    "sql_script = f\"\"\"\n",
    "SELECT dataset.name, sample.id, sample.filename, sample.local_trimmed_path, sample.original_text, sample.asr_text, sample.wer, sample.trimmed_audio_duration as duration\n",
    "FROM sample\n",
    "JOIN dataset ON sample.dataset_id = dataset.id\n",
    "WHERE dataset.name LIKE '%' || '{dataset}' || '%';\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Execute the SQL script into pandas dataframe with column names\n",
    "df = pd.read_sql_query(sql_script, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by name and then create a dict of the grouped dataframes\n",
    "\n",
    "df_dict = {k: v for k, v in df.groupby(\"name\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matched_list = []\n",
    "for df_name, df_sentences in df_dict.items():\n",
    "    print(f\"Processing {df_name}\")\n",
    "    df_sentences = df_sentences.reset_index(drop=True)\n",
    "\n",
    "    sentences = {}\n",
    "    inverseSentences = {}\n",
    "\n",
    "    segments = {}\n",
    "\n",
    "    print(f\"There are {len(df_sentences)} sentences in this range\")\n",
    "    for index, row in df_sentences.iterrows():\n",
    "        sentenceNum = int(index)\n",
    "        sentence = row[\"original_text\"]\n",
    "        sentences[sentenceNum] = sentence\n",
    "\n",
    "        segments[sentenceNum] = row\n",
    "        if sentence not in inverseSentences:\n",
    "            inverseSentences[sentence] = sentenceNum\n",
    "        else:\n",
    "            tmp = sentence\n",
    "            while tmp in inverseSentences:\n",
    "                tmp += \" _\"\n",
    "            inverseSentences[tmp] = sentenceNum\n",
    "\n",
    "    \n",
    "    sentenceNumber = -1\n",
    "\n",
    "    segments_list = [v for k, v in segments.items()]\n",
    "    sentences_list = [v for k, v in sentences.items()]\n",
    "    distances_matrix = np.ones((len(segments_list), len(sentences))) * 1000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for ik in tqdm(range(len(segments_list))):\n",
    "        # for jk in range(ik-500, min(len(segments_list), ik+500)):\n",
    "        for jk in range(len(sentences_list)):\n",
    "            try:\n",
    "                distances_matrix[ik, jk] = edit_distance(segments_list[ik][\"asr_text\"], sentences_list[jk]) / min(len(segments_list[ik][\"asr_text\"]), len(sentences_list[jk]))\n",
    "            except:\n",
    "                distances_matrix[ik, jk] = np.inf\n",
    "\n",
    "    # get the best match for each segment\n",
    "    best_matches = np.argmin(distances_matrix, axis=1)\n",
    "    best_matched_sentences = [sentences_list[k] for k in best_matches]\n",
    "\n",
    "\n",
    "\n",
    "    # # make a dataframe\n",
    "    rows = []\n",
    "    best_matched_sentences = [sentences_list[k] for k in best_matches]\n",
    "\n",
    "    # print the results\n",
    "    for ik in tqdm(range(len(segments_list))):\n",
    "        asr = segments_list[ik][\"asr_text\"]\n",
    "        sentence = best_matched_sentences[ik]\n",
    "        ed_dist = distances_matrix[ik, best_matches[ik]]\n",
    "        try:\n",
    "            len_dif = abs(len(asr) - len(sentence)) / min(len(asr), len(sentence))\n",
    "        except:\n",
    "            len_dif = np.inf\n",
    "        sentenceNumber = inverseSentences[sentence]\n",
    "        if ed_dist < 0.25 and len_dif < 0.15:\n",
    "            status = \"assigned\"\n",
    "        else:\n",
    "            status = \"not_assigned\"\n",
    "\n",
    "        row = {\n",
    "            \"status\": status,\n",
    "            \"originalNumber\": ik,\n",
    "            \"original_id\": segments_list[ik][\"id\"],\n",
    "            \"assigned_id\": segments[sentenceNumber][\"id\"],\n",
    "            \"original_sentence\": sentences_list[ik],\n",
    "            \"assigned_sentence\": sentence,\n",
    "            \"ed_dist\": ed_dist,\n",
    "            \"len_dif\": len_dif,\n",
    "        }\n",
    "\n",
    "        row.update(segments_list[ik])\n",
    "        rows.append(row)\n",
    "    # if there is inf  drop it\n",
    "    df_matched_ = pd.DataFrame(rows)\n",
    "    df_matched_ = df_matched_[df_matched_[\"ed_dist\"] != np.inf]\n",
    "    \n",
    "    diff = df_matched_[df_matched_.original_id != df_matched_.assigned_id]\n",
    "    diff = diff[diff.status==\"assigned\"]\n",
    "    # drop duplicates with higher edit distance\n",
    "    diff = diff.sort_values(\"ed_dist\").drop_duplicates(\"assigned_id\", keep=\"first\")\n",
    "    df_matched_list.append(diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "df_matched = pd.concat(df_matched_list)\n",
    "df_matched = df_matched[df_matched.duplicated(\"assigned_id\", keep=False)].sort_values([\"assigned_id\", \"ed_dist\"]).drop_duplicates(\"assigned_id\", keep=\"first\")\n",
    "\n",
    "\n",
    "df_matched.to_csv(\"matched.csv\", index=False)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff.to_csv(\"diff.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign the closest match to each row in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a temporary folder to store the audio files\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import tempfile\n",
    "import shutil \n",
    "from src.utils.utils import calculate_wer\n",
    "import traceback\n",
    "\n",
    "from src.paths import paths\n",
    "from src.logger import root_logger\n",
    "import boto3\n",
    "\n",
    "app_logger = root_logger.getChild(\"rematch\")\n",
    "\n",
    "BASE_DIR = paths.PROJECT_ROOT_DIR\n",
    "\n",
    "if load_dotenv(os.path.join(BASE_DIR, \"vars.env\")):\n",
    "    app_logger.info(\"Loaded env vars from vars.env\")\n",
    "else:\n",
    "    app_logger.error(\"Failed to load env vars from vars.env\")\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "\n",
    "temp_dir = tempfile.TemporaryDirectory()\n",
    "\n",
    "# make raw and trimmed paths \n",
    "if not os.path.exists(os.path.join(temp_dir.name, \"raw\")):\n",
    "    os.makedirs(os.path.join(temp_dir.name, \"raw\"))\n",
    "\n",
    "if not os.path.exists(os.path.join(temp_dir.name, \"trimmed\")):\n",
    "    os.makedirs(os.path.join(temp_dir.name, \"trimmed\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matched = pd.read_csv(\"matched-Italian.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matched.drop_duplicates(\"assigned_id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_sample(assigned_sample, original_sample):\n",
    "    # copy the wav file from assigned_sample to temp_dir\n",
    "    temp_sample = original_sample.copy()\n",
    "    \n",
    "    # copy the wav file from assigned_sample to temp_dir ot local path and trimmed path \n",
    "    orig_local_path = original_sample[\"local_path\"]\n",
    "    orig_local_trimmed_path = original_sample[\"local_trimmed_path\"]\n",
    "\n",
    "    filename = os.path.basename(assigned_sample[\"local_path\"])\n",
    "\n",
    "    # copy from assigned_local_path to temp_dir with the filename\n",
    "    shutil.copy(orig_local_path, os.path.join(temp_dir.name, \"raw\", filename))\n",
    "    shutil.copy(orig_local_trimmed_path, os.path.join(temp_dir.name, \"trimmed\", filename))\n",
    "\n",
    "    temp_sample[\"filename\"] = filename\n",
    "    temp_sample[\"local_path\"] = os.path.join(temp_dir.name, \"raw\", filename)\n",
    "    temp_sample[\"local_trimmed_path\"] = os.path.join(temp_dir.name, \"trimmed\", filename)\n",
    "    temp_sample[\"original_text\"] = assigned_sample[\"original_text\"]\n",
    "    temp_sample[\"sentence_type\"] = assigned_sample[\"sentence_type\"]\n",
    "    temp_sample[\"sentence_length\"] = assigned_sample[\"sentence_length\"]\n",
    "\n",
    "    temp_sample[\"duration\"] = assigned_sample[\"duration\"]\n",
    "\n",
    "    temp_sample[\"wer\"] =  round(float(calculate_wer(temp_sample[\"original_text\"], temp_sample[\"asr_text\"])), 2)\n",
    "    return temp_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each assigned sentence,  get the original sentence and the assigned sentence in the dataframe\n",
    "updated_samples = []\n",
    "all_samples = []\n",
    "for i, row in df_matched.iterrows():\n",
    "\n",
    "    sql_script = f\"\"\"\n",
    "    select * from sample where id = {row.original_id} or id = {row.assigned_id};\n",
    "    \"\"\"\n",
    "    all_samples.append(row.original_id)\n",
    "    all_samples.append(row.assigned_id)\n",
    "    # Execute the SQL script into pandas dataframe with column names\n",
    "    df_tmp = pd.read_sql_query(sql_script, conn)\n",
    "\n",
    "    if len(df_tmp) != 2:\n",
    "        print(f\"error in {row.original_id} or {row.assigned_id}\")\n",
    "        continue\n",
    "    \n",
    "    assigned_sample = df_tmp[df_tmp.id == row.assigned_id].iloc[0].to_dict()\n",
    "    original_sample = df_tmp[df_tmp.id == row.original_id].iloc[0].to_dict()\n",
    "\n",
    "    try:\n",
    "        temp_sample = update_sample(assigned_sample, original_sample)\n",
    "        updated_samples.append(temp_sample)\n",
    "    except:\n",
    "        app_logger.error(f\"error in {row.original_id} or {row.assigned_id}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = list(set(all_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\", aws_access_key_id=os.environ.get(\"AWS_ACCESS_KEY_ID\"), aws_secret_access_key=os.environ.get(\"AWS_SECRET_ACCESS_KEY\"))\n",
    "bucket_name = os.environ.get(\"S3_BUCKET_NAME\")\n",
    "dataset_dir = os.environ.get(\"S3_DATASET_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# delete all samples from sql \n",
    "\n",
    "sql_script = f\"\"\"   \n",
    "select * from sample where id in ({\",\".join([str(k) for k in all_samples])});\n",
    "\"\"\"\n",
    "\n",
    "# get each sample and delete  \n",
    "df_tmp = pd.read_sql_query(sql_script, conn)\n",
    "\n",
    "cursor = conn.cursor()\n",
    "for i, row in df_tmp.iterrows():\n",
    "    try:\n",
    "        # delete from sql\n",
    "        sql_script = f\"\"\"\n",
    "        delete from sample where id = {row.id};\n",
    "        \"\"\"\n",
    "        cursor.execute(sql_script)\n",
    "        conn.commit()\n",
    "        app_logger.info(f\"Deleted sample {row.id} from sql\")\n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Failed to delete sample {row.id} from sql\")\n",
    "        continue\n",
    "    try:\n",
    "        # delete the local files\n",
    "        os.remove(row.local_path)\n",
    "        os.remove(row.local_trimmed_path)\n",
    "        # delete the s3 from full paths:  looks like 's3://user-ahmet/tts-data/Spanish(Violeta) Deliverable 7/raw/ES00110380.wav'\n",
    "        s3.delete_object(Bucket=bucket_name, Key=row.local_path.replace(f\"s3://{bucket_name}/{dataset_dir}/\", \"\"))\n",
    "        s3.delete_object(Bucket=bucket_name, Key=row.local_trimmed_path.replace(f\"s3://{bucket_name}/{dataset_dir}/\", \"\"))\n",
    "        app_logger.info(f\"Deleted sample {row.id} from sql, local and s3\")\n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Failed to delete sample {row.id} from sql, local and s3\")\n",
    "        app_logger.error(traceback.format_exc())\n",
    "        continue\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(updated_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cursor = conn.cursor()\n",
    "# insert the new samples into sql\n",
    "for sample in updated_samples:\n",
    "    # insert the new samples into sql :\n",
    "\n",
    "    # sample looks like this {'dataset_id': 35, ... }\n",
    "\n",
    "    old_filename = sample['s3RawPath'].split(\"/\")[-1]\n",
    "\n",
    "    # replace the filename with the new one\n",
    "    sample['s3RawPath'] = sample['s3RawPath'].replace(old_filename, sample['filename'])\n",
    "    sample['s3TrimmedPath'] = sample['s3TrimmedPath'].replace(old_filename, sample['filename'])\n",
    "\n",
    "    # update the paths\n",
    "    local_folder = sample['s3RawPath']\n",
    "    # remove the filename\n",
    "    local_path = local_folder.replace(f\"s3://{bucket_name}/\", \"/data/tts-qa/\")\n",
    "\n",
    "    local_trimmed_folder = sample['s3TrimmedPath']\n",
    "    # remove the filename\n",
    "    local_trimmed_folder = local_trimmed_folder.replace(f\"s3://{bucket_name}/\", \"/data/tts-qa/\")\n",
    "\n",
    "    # copy paste the files to local\n",
    "    if not os.path.exists(os.path.dirname(sample['local_path'])):\n",
    "        app_logger.info(f\"Folder does not exists {os.path.dirname(sample['local_path'])}\")\n",
    "        continue\n",
    "    if not os.path.exists(os.path.dirname(sample['local_trimmed_path'])):\n",
    "        app_logger.info(f\"Folder does not exists {os.path.dirname(sample['local_trimmed_path'])}\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # copy local files to \n",
    "        shutil.copyfile(sample['local_path'], local_path)\n",
    "        shutil.copyfile(sample['local_trimmed_path'], local_trimmed_folder)\n",
    "\n",
    "        # update the local paths\n",
    "        sample['local_path'] = local_path\n",
    "        sample['local_trimmed_path'] = local_trimmed_folder\n",
    "\n",
    "        # upload the files to the s3 path \n",
    "        s3.upload_file(sample['local_path'], bucket_name, sample['s3RawPath'].replace(f\"s3://{bucket_name}/\", \"\"))\n",
    "        s3.upload_file(sample['local_trimmed_path'], bucket_name, sample['s3TrimmedPath'].replace(f\"s3://{bucket_name}/\", \"\"))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    app_logger.info(f\"Uploaded sample {sample['id']} to s3\")\n",
    "    try:\n",
    "        sql_script = f\"\"\"\n",
    "                insert into sample (dataset_id, filename, local_path, local_trimmed_path, \"s3RawPath\", \"s3TrimmedPath\", original_text, asr_text, duration, trimmed_audio_duration, sentence_type, sentence_length, sampling_rate, sample_format, \"isPCM\", n_channel, format, peak_volume_db, size, \"isValid\", trim_start, trim_end, longest_pause, wer)\n",
    "                values ({sample['dataset_id']}, '{sample['filename']}', '{sample['local_path']}', '{sample['local_trimmed_path']}', '{sample['s3RawPath']}', '{sample['s3TrimmedPath']}', '{sample['original_text'].replace(\"'\", \"''\")}', '{sample['asr_text'].replace(\"'\", \"''\")}',{sample['duration']}, {sample['trimmed_audio_duration']}, '{sample['sentence_type']}', {sample['sentence_length']}, {sample['sampling_rate']}, '{sample['sample_format']}', {sample['isPCM']}, {sample['n_channel']}, '{sample['format']}', {sample['peak_volume_db']}, {sample['size']}, {sample['isValid']}, {sample['trim_start']}, {sample['trim_end']}, {sample['longest_pause']}, {sample['wer']});\n",
    "                \"\"\"\n",
    "        cursor.execute(sql_script)\n",
    "        conn.commit()\n",
    "\n",
    "        app_logger.info(f\"Inserted sample with informations {sample} into sql\")\n",
    "        app_logger.info(f\"Inserted sample {sample['id']} into sql\")\n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Failed to insert sample {sample['id']} into sql\")\n",
    "        app_logger.error(traceback.format_exc())\n",
    "        continue\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the cursor and connection\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save updated_samples \n",
    "import json\n",
    "with open(\"updated_samples.json\", \"w\") as f:\n",
    "    json.dump(updated_samples, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the updated_samples\n",
    "import json\n",
    "with open(\"updated_samples.json\", \"r\") as f:\n",
    "    updated_samples = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "corpus-insight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
