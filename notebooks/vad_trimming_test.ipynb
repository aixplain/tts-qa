{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "import editdistance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyannote.audio import Model\n",
    "from pyannote.audio.pipelines import VoiceActivityDetection\n",
    "from pyannote.core import notebook, Segment\n",
    "from pydub import AudioSegment\n",
    "from tqdm import tqdm\n",
    "from whisper_model import WhisperASR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.io.wavfile as wav\n",
    "import scipy.signal as signal\n",
    "\n",
    "import tempfile\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "from IPython.display import Audio\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim the audio using start end end time in secs\n",
    "def trim_audio(path, start, end, out_path):\n",
    "    sound = AudioSegment.from_file(path, format=\"wav\")\n",
    "    # make sure that the start and end are in between the audio duration\n",
    "    start = max(0, start)\n",
    "    end = min(end, len(sound) / 1000)\n",
    "    trimmed_sound = sound[start * 1000 : end * 1000]\n",
    "    trimmed_sound.export(out_path, format=\"wav\")\n",
    "    return out_path, start, end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files_folder = \"/data/tts-qa/tts-data/French(Dorsaf)/trimmed\"\n",
    "files_folder = \"/data/tts-qa/tts-data/German(Dorothee)/trimmed\"\n",
    "# files_folder = \"/data/tts-qa/tts-data/English(Melynda)/trimmed\"\n",
    "# files_folder = \"/data/tts-qa/tts-data/Italian(Martina) Deliverable 3/trimmed\"\n",
    "# files_folder = \"/data/tts-qa/tts-data/Spanish(Violeta) Deliverable 3/trimmed\"\n",
    "\n",
    "\n",
    "files = glob(os.path.join(files_folder, \"*.wav\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select  files\n",
    "\n",
    "np.random.seed(10)\n",
    "selected_files = np.random.choice(files, 50, replace=False)\n",
    "\n",
    "# selected_files = [\n",
    "#     \"/data/tts-qa/tts-data/French(Dorsaf)/trimmed/FR00000026.wav\",\n",
    "#     \"/data/tts-qa/tts-data/French(Dorsaf)/trimmed/FR00000032.wav\",\n",
    "#     \"/data/tts-qa/tts-data/French(Dorsaf)/trimmed/FR00000033.wav\",\n",
    "#     \"/data/tts-qa/tts-data/French(Dorsaf)/trimmed/FR00000035.wav\",\n",
    "#     \"/data/tts-qa/tts-data/French(Dorsaf)/trimmed/FR00001001.wav\",\n",
    "# ]\n",
    "\n",
    "\n",
    "# rare_cases = [\n",
    "#     \"/data/tts-qa/tts-data/French(Dorsaf)/raw/FR00000907.wav\",\n",
    "#     \"/data/tts-qa/tts-data/French(Dorsaf)/raw/FR00000115.wav\",\n",
    "#     \"/data/tts-qa/tts-data/French(Dorsaf)/raw/FR00000169.wav\",\n",
    "#     \"/data/tts-qa/tts-data/French(Dorsaf)/raw/FR00000952.wav\",\n",
    "#     \"/data/tts-qa/tts-data/French(Dorsaf)/raw/FR00000584.wav\",\n",
    "#     \"/data/tts-qa/tts-data/French(Dorsaf)/raw/FR00000911.wav\",\n",
    "#     \"/data/tts-qa/tts-data/French(Dorsaf)/raw/FR00001238.wav\",\n",
    "#     \"/data/tts-qa/tts-data/French(Dorsaf)/raw/FR00000979.wav\",\n",
    "# ]\n",
    "# selected_files= rare_cases\n",
    "\n",
    "# selected_files = [\n",
    "#     \"/data/tts-qa/tts-data/German(Dorothee)/trimmed/DE00080623.wav\",\n",
    "#     \"/data/tts-qa/tts-data/German(Dorothee)/trimmed/DE00069958.wav\",\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "import scipy.signal as signal\n",
    "import matplotlib.pyplot as plt\n",
    "from pydub import AudioSegment\n",
    "from pyannote.audio import Model\n",
    "from pyannote.audio.pipelines import VoiceActivityDetection\n",
    "from copy import deepcopy\n",
    "\n",
    "class CustomVAD:\n",
    "    SAMPLING_RATE = 16000\n",
    "    PADDING = 0.025\n",
    "    ENERGY_THRESHOLD = 500000\n",
    "    WINDOW_SIZE = 0.02\n",
    "\n",
    "    def __init__(self, pyannote_model_path, silero_model_path, hyper_parameters=None):\n",
    "        self.pyannote_model = Model.from_pretrained(pyannote_model_path, use_auth_token=\"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\")\n",
    "        self.silero_model, self.silero_utils = torch.hub.load(repo_or_dir=silero_model_path, model='silero_vad', force_reload=True, onnx=False)\n",
    "        self.pipeline = VoiceActivityDetection(segmentation=self.pyannote_model)\n",
    "        if hyper_parameters is None:\n",
    "            hyper_parameters = {\n",
    "                \"onset\": 0.5, \"offset\": 0.5, \"min_duration_on\": 0.0, \"min_duration_off\": 0.05\n",
    "            }\n",
    "        self.pipeline.instantiate(hyper_parameters)\n",
    "\n",
    "        (self.get_speech_timestamps,\n",
    "        self.save_audio,\n",
    "        self.read_audio,\n",
    "        self.VADIterator,\n",
    "        self.collect_chunks) = self.silero_utils\n",
    "\n",
    "    @staticmethod\n",
    "    def pad(waveform, segment):\n",
    "        start, end = segment\n",
    "        start = max(0, start - CustomVAD.PADDING)\n",
    "        end = min(len(waveform) / CustomVAD.SAMPLING_RATE, end + CustomVAD.PADDING)\n",
    "        return start, end\n",
    "\n",
    "    def run_pyannote_vad(self, file):\n",
    "        vad_segments = self.pipeline(file)\n",
    "        pyannote_timeline = vad_segments.get_timeline().support()\n",
    "        response_timeline = [(segment.start, segment.end) for segment in pyannote_timeline]\n",
    "        # get start of first and end of last\n",
    "        if len(response_timeline) > 0:\n",
    "            start = response_timeline[0][0]\n",
    "            end = response_timeline[-1][1]\n",
    "        else:\n",
    "            start = 0\n",
    "            end = 0\n",
    "        response_timeline = (start, end)\n",
    "        return response_timeline\n",
    "\n",
    "    def run_silero_vad(self, file):\n",
    "        wav = self.read_audio(file, sampling_rate=CustomVAD.SAMPLING_RATE)\n",
    "        silero_timeline = self.get_speech_timestamps(wav, self.silero_model, sampling_rate=CustomVAD.SAMPLING_RATE)\n",
    "        silero_timeline = [(segment[\"start\"] / CustomVAD.SAMPLING_RATE, segment[\"end\"] / CustomVAD.SAMPLING_RATE) for segment in silero_timeline]\n",
    "        # get start of first and end of last\n",
    "        if len(silero_timeline) > 0:\n",
    "            start = silero_timeline[0][0]\n",
    "            end = silero_timeline[-1][1]\n",
    "        else:\n",
    "            start = 0\n",
    "            end = 0\n",
    "        silero_timeline = (start, end)\n",
    "        return silero_timeline\n",
    "\n",
    "    def my_custom_vad(self, pyannote_segment, silero_segment, waveform):\n",
    "        merged_timeline = []\n",
    "        # Your logic for merging or comparing pyannote and silero timelines\n",
    "        pyannote_start, pyannote_end = list(deepcopy(pyannote_segment))\n",
    "        silero_start, silero_end = list(deepcopy(silero_segment))\n",
    "\n",
    "        # If the segments are close enough, merge them\n",
    "        if abs(pyannote_start - silero_start) < 0.05:\n",
    "            merged_start = min(pyannote_start, silero_start)\n",
    "        else:\n",
    "            # Divide the segment into smaller windows and check energy\n",
    "            start_start = min(pyannote_start, silero_start)\n",
    "            start_end = max(pyannote_start, silero_start)\n",
    "            merged_timeline = []\n",
    "            merged_start = start_end\n",
    "            num_windows = int((start_end - start_start) / CustomVAD.WINDOW_SIZE)\n",
    "            for i in range(num_windows):\n",
    "                window_start = int((start_start + i * CustomVAD.WINDOW_SIZE) * CustomVAD.SAMPLING_RATE)\n",
    "                window_end = int(window_start + CustomVAD.WINDOW_SIZE * CustomVAD.SAMPLING_RATE)\n",
    "                window_samples = waveform[window_start:window_end]\n",
    "                window_energy = np.sum(window_samples ** 2) / len(window_samples)\n",
    "\n",
    "                if window_energy > CustomVAD.ENERGY_THRESHOLD:\n",
    "                    merged_timeline.append((start_start + i * CustomVAD.WINDOW_SIZE, start_start + (i + 1) * CustomVAD.WINDOW_SIZE, window_energy))\n",
    "            if len(merged_timeline)>0:\n",
    "                merged_start = merged_timeline[0][0]\n",
    "            \n",
    "        if abs(pyannote_end - silero_end) < 0.05:\n",
    "            merged_end = max(pyannote_end, silero_end)\n",
    "        else:\n",
    "            end_start = min(pyannote_end, silero_end)\n",
    "            end_end = max(pyannote_end, silero_end)\n",
    "            merged_timeline = []\n",
    "            merged_end = end_start\n",
    "            num_windows = int((end_end - end_start) / CustomVAD.WINDOW_SIZE)\n",
    "            for i in range(num_windows):\n",
    "                window_start = int((end_start + i * CustomVAD.WINDOW_SIZE) * CustomVAD.SAMPLING_RATE)\n",
    "                window_end = int(window_start + CustomVAD.WINDOW_SIZE * CustomVAD.SAMPLING_RATE)\n",
    "                window_samples = waveform[window_start:window_end]\n",
    "                window_energy = np.sum(window_samples ** 2) / len(window_samples)\n",
    "\n",
    "                if window_energy > CustomVAD.ENERGY_THRESHOLD:\n",
    "                    merged_timeline.append((end_start + i * CustomVAD.WINDOW_SIZE, end_start + (i + 1) * CustomVAD.WINDOW_SIZE, window_energy))\n",
    "            if len(merged_timeline)>0:\n",
    "                merged_end = merged_timeline[-1][1]\n",
    "\n",
    "        custom_segment = (merged_start, merged_end)\n",
    "        \n",
    "        return custom_segment\n",
    "\n",
    "    def process_file(self, file):\n",
    "        audio = AudioSegment.from_file(file, format=\"wav\")\n",
    "        waveform = np.array(audio.get_array_of_samples())\n",
    "        original_sampling_rate = audio.frame_rate\n",
    "        resampled_waveform = signal.resample(waveform, int(len(waveform) * CustomVAD.SAMPLING_RATE / original_sampling_rate))\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            temp_wav_file = os.path.join(temp_dir, \"temp_audio.wav\")\n",
    "            wav.write(temp_wav_file, CustomVAD.SAMPLING_RATE, resampled_waveform.astype('int16'))\n",
    "            pyannote_segment = self.run_pyannote_vad(file)\n",
    "            silero_segment = self.run_silero_vad(file)\n",
    "        print (pyannote_segment, silero_segment)\n",
    "        pyannote_segment = self.pad(resampled_waveform, pyannote_segment)\n",
    "        silero_segment = self.pad(resampled_waveform, silero_segment)\n",
    "\n",
    "        custom_segment = self.my_custom_vad(pyannote_segment, silero_segment, resampled_waveform)\n",
    "        custom_segment = self.pad(resampled_waveform, custom_segment)\n",
    "        response = {\n",
    "            \"resampled_waveform\": resampled_waveform,\n",
    "            \"pyannote_segment\": pyannote_segment,\n",
    "            \"silero_segment\": silero_segment,\n",
    "            \"custom_segment\": custom_segment,\n",
    "        }\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_custom_vad = CustomVAD(pyannote_model_path=\"pyannote/segmentation\", silero_model_path=\"snakers4/silero-vad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for selected files and plot\n",
    "for file in selected_files:\n",
    "    response = my_custom_vad.process_file(file)\n",
    "\n",
    "    # load the waveform\n",
    "    waveform = response[\"resampled_waveform\"]\n",
    "    # Calculate time vector\n",
    "    time_vector = np.linspace(0, len(waveform) / my_custom_vad.SAMPLING_RATE, num=len(waveform))\n",
    "\n",
    "    # plot \n",
    "    fig = plt.figure(figsize=(20, 5), dpi=50)\n",
    "    plt.yticks([])\n",
    "    plt.plot(time_vector, waveform)\n",
    "\n",
    "    # add the filename as title\n",
    "    plt.title(os.path.basename(file))\n",
    "\n",
    "    # plot pyannote\n",
    "    plt.axvspan(response[\"pyannote_segment\"][0], response[\"pyannote_segment\"][1], color=\"green\", alpha=0.3, label=\"PYANNOTE\")\n",
    "\n",
    "    # plot silero\n",
    "    plt.axvspan(response[\"silero_segment\"][0], response[\"silero_segment\"][1], color=\"red\", alpha=0.3, label=\"SILERO-VAD\")\n",
    "\n",
    "    # plot custom\n",
    "    plt.axvspan(response[\"custom_segment\"][0], response[\"custom_segment\"][1], color=\"black\", alpha=0.3, label=\"MY_CUSTOM_VAD\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "corpus-insight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
