{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "import editdistance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyannote.audio import Model\n",
    "from pyannote.audio.pipelines import VoiceActivityDetection\n",
    "from pyannote.core import notebook, Segment\n",
    "from pydub import AudioSegment\n",
    "from tqdm import tqdm\n",
    "from whisper_model import WhisperASR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "SAMPLING_RATE = 16000\n",
    "\n",
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "from IPython.display import Audio\n",
    "from pprint import pprint\n",
    "USE_ONNX = False # change this to True if you want to test onnx model\n",
    "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "                              model='silero_vad',\n",
    "                              force_reload=True,\n",
    "                              onnx=USE_ONNX)\n",
    "\n",
    "(get_speech_timestamps,\n",
    " save_audio,\n",
    " read_audio,\n",
    " VADIterator,\n",
    " collect_chunks) = utils\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "modelPyannote = Model.from_pretrained(\"pyannote/segmentation\", use_auth_token=\"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\")\n",
    "\n",
    "pipeline = VoiceActivityDetection(segmentation=modelPyannote)\n",
    "HYPER_PARAMETERS = {\n",
    "    # onset/offset activation thresholds\n",
    "    \"onset\": 0.5,\n",
    "    \"offset\": 0.5,\n",
    "    # remove speech regions shorter than that many seconds.\n",
    "    \"min_duration_on\": 0.0,\n",
    "    # fill non-speech regions shorter than that many seconds.\n",
    "    \"min_duration_off\": 0.05,\n",
    "}\n",
    "pipeline.instantiate(HYPER_PARAMETERS)\n",
    "\n",
    "padding = 0.025\n",
    "\n",
    "def edit_distance(s1, s2):\n",
    "    return editdistance.eval(s1, s2)\n",
    "\n",
    "\n",
    "def format_int(i):\n",
    "    return str(i).zfill(8)\n",
    "\n",
    "\n",
    "# trim the audio using start end end time in secs\n",
    "def trim_audio(path, start, end, out_path):\n",
    "    sound = AudioSegment.from_file(path, format=\"wav\")\n",
    "    # make sure that the start and end are in between the audio duration\n",
    "    start = max(0, start)\n",
    "    end = min(end, len(sound) / 1000)\n",
    "    trimmed_sound = sound[start * 1000 : end * 1000]\n",
    "    trimmed_sound.export(out_path, format=\"wav\")\n",
    "    return out_path, start, end\n",
    "\n",
    "def run_pyannote_vad(file):\n",
    "    vad_segments = pipeline(file)\n",
    "    pyannote_timeline = vad_segments.get_timeline().support()\n",
    "    response_timeline = []\n",
    "    for segment in pyannote_timeline:\n",
    "        start, end = list(segment)\n",
    "        response_timeline.append((start, end))\n",
    "    # get start of first and end of last\n",
    "    if len(response_timeline) > 0:\n",
    "        start = response_timeline[0][0]\n",
    "        end = response_timeline[-1][1]\n",
    "    else:\n",
    "        start = 0\n",
    "        end = 0\n",
    "    response_timeline = [(start, end)]\n",
    "    return response_timeline\n",
    "\n",
    "def run_silerio_vad(file):\n",
    "    wav = read_audio(file, sampling_rate=SAMPLING_RATE)\n",
    "    # get speech timestamps from full audio file\n",
    "    silerio_timeline = get_speech_timestamps(wav, model, sampling_rate=SAMPLING_RATE)\n",
    "    # show the timestamps as s\n",
    "    silerio_timeline = [(segment[\"start\"] / SAMPLING_RATE, segment[\"end\"] / SAMPLING_RATE) for segment in silerio_timeline]\n",
    "\n",
    "    # get start of first and end of last\n",
    "    if len(silerio_timeline) > 0:\n",
    "        start = silerio_timeline[0][0]\n",
    "        end = silerio_timeline[-1][1]\n",
    "    else:\n",
    "        start = 0\n",
    "        end = 0\n",
    "    silerio_timeline = [(start, end)]\n",
    "    return silerio_timeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_folder = \"/data/tts-qa/tts-data/French(Dorsaf)/raw\"\n",
    "files = glob(os.path.join(files_folder, \"*.wav\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select 10 files\n",
    "\n",
    "np.random.seed(10)\n",
    "selected_files = np.random.choice(files, 50, replace=False)\n",
    "\n",
    "selected_files = [\n",
    "    \"/data/tts-qa/tts-data/French(Dorsaf)/raw/FR00000026.wav\",\n",
    "    \"/data/tts-qa/tts-data/French(Dorsaf)/raw/FR00000032.wav\",\n",
    "    \"/data/tts-qa/tts-data/French(Dorsaf)/raw/FR00000033.wav\",\n",
    "    \"/data/tts-qa/tts-data/French(Dorsaf)/raw/FR00000035.wav\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def my_custom_vad(pyannote_timeline, silerio_timeline, waveform, sample_rate, energy_threshold=0.9, window_size=0.02):\n",
    "    merged_timeline = []\n",
    "\n",
    "    # Your logic for merging or comparing pyannote and silerio timelines\n",
    "    for pyannote_segment in pyannote_timeline:\n",
    "        for silerio_segment in silerio_timeline:\n",
    "            pyannote_start, pyannote_end = list(pyannote_segment)\n",
    "            silerio_start, silerio_end = list(silerio_segment)\n",
    "\n",
    "            # If the segments are close enough, merge them\n",
    "            if abs(pyannote_start - silerio_start) < 0.2 and abs(pyannote_end - silerio_end) < 0.2:\n",
    "                merged_start = min(pyannote_start, silerio_start)\n",
    "                merged_end = max(pyannote_end, silerio_end)\n",
    "                merged_timeline.append((merged_start, merged_end))\n",
    "            else:\n",
    "                # Divide the segment into smaller windows and check energy\n",
    "                segment_start = min(pyannote_start, silerio_start)\n",
    "                segment_end = max(pyannote_end, silerio_end)\n",
    "                num_windows = int((segment_end - segment_start) / window_size)\n",
    "                for i in range(num_windows):\n",
    "                    window_start = int((segment_start + i * window_size) * sample_rate)\n",
    "                    window_end = int(window_start + window_size * sample_rate)\n",
    "                    window_samples = waveform[window_start:window_end]\n",
    "                    window_energy = np.sum(window_samples ** 2) / len(window_samples)\n",
    "\n",
    "                    if window_energy > energy_threshold:\n",
    "                        merged_timeline.append((segment_start + i * window_size, segment_start + (i + 1) * window_size, window_energy))\n",
    "    \n",
    "    custom_timeline = [(merged_timeline[0][0], merged_timeline[-1][1])]\n",
    "    return custom_timeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for file in selected_files:\n",
    "    # load the waveform\n",
    "    audio = AudioSegment.from_file(file, format=\"wav\")\n",
    "    waveform = np.array(audio.get_array_of_samples())\n",
    "    sample_rate = audio.frame_rate\n",
    "\n",
    "    pyannote_timeline = run_pyannote_vad(file)\n",
    "    silerio_timeline = run_silerio_vad(file)\n",
    "    \n",
    "\n",
    "    custom_timeline = my_custom_vad(pyannote_timeline, silerio_timeline, waveform, sample_rate)\n",
    "\n",
    "    audio = AudioSegment.from_file(file, format=\"wav\")\n",
    "    waveform = np.array(audio.get_array_of_samples())\n",
    "    # Calculate time vector\n",
    "    time_vector = np.linspace(0, len(waveform) / sample_rate, num=len(waveform))\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 5))\n",
    "    plt.yticks([])\n",
    "    plt.plot(time_vector, waveform)\n",
    "    # add the filename as title\n",
    "    plt.title(os.path.basename(file))\n",
    "#     for segment in silerio_timeline:\n",
    "#         start, end = list(segment)\n",
    "#         start = max(0, start-padding)\n",
    "#         end = min(len(waveform) / sample_rate, end+padding)\n",
    "#         plt.axvspan(start, end , color=\"red\", alpha=0.5, label=\"SILERIO-VAD\")\n",
    "    for segment in pyannote_timeline:\n",
    "        start, end = list(segment)\n",
    "        start = max(0, start-padding)\n",
    "        end = min(len(waveform) / sample_rate, end+padding)\n",
    "        plt.axvspan(start, end, color=\"green\", alpha=0.3, label=\"PYANNOTE\")\n",
    "    \n",
    "    \n",
    "    for segment in custom_timeline:\n",
    "        start, end = list(segment)\n",
    "        plt.axvspan(start, end, color=\"red\", alpha=0.3, label=\"MY_CUSTOM_VAD\")\n",
    "\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "corpus-insight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
