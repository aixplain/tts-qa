{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "import editdistance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyannote.audio import Model\n",
    "from pyannote.audio.pipelines import VoiceActivityDetection\n",
    "from pyannote.core import notebook, Segment\n",
    "from pydub import AudioSegment\n",
    "from tqdm import tqdm\n",
    "from whisper_model import WhisperASR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.io.wavfile as wav\n",
    "import scipy.signal as signal\n",
    "\n",
    "import tempfile\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "from IPython.display import Audio\n",
    "from pprint import pprint\n",
    "USE_ONNX = False # change this to True if you want to test onnx model\n",
    "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "                              model='silero_vad',\n",
    "                              force_reload=True,\n",
    "                              onnx=USE_ONNX)\n",
    "\n",
    "(get_speech_timestamps,\n",
    " save_audio,\n",
    " read_audio,\n",
    " VADIterator,\n",
    " collect_chunks) = utils\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "modelPyannote = Model.from_pretrained(\"pyannote/segmentation\", use_auth_token=\"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\")\n",
    "\n",
    "pipeline = VoiceActivityDetection(segmentation=modelPyannote)\n",
    "HYPER_PARAMETERS = {\n",
    "    # onset/offset activation thresholds\n",
    "    \"onset\": 0.5,\n",
    "    \"offset\": 0.5,\n",
    "    # remove speech regions shorter than that many seconds.\n",
    "    \"min_duration_on\": 0.0,\n",
    "    # fill non-speech regions shorter than that many seconds.\n",
    "    \"min_duration_off\": 0.05,\n",
    "}\n",
    "pipeline.instantiate(HYPER_PARAMETERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "padding = 0.025\n",
    "SAMPLING_RATE = 16000\n",
    "def edit_distance(s1, s2):\n",
    "    return editdistance.eval(s1, s2)\n",
    "\n",
    "\n",
    "def format_int(i):\n",
    "    return str(i).zfill(8)\n",
    "\n",
    "\n",
    "# trim the audio using start end end time in secs\n",
    "def trim_audio(path, start, end, out_path):\n",
    "    sound = AudioSegment.from_file(path, format=\"wav\")\n",
    "    # make sure that the start and end are in between the audio duration\n",
    "    start = max(0, start)\n",
    "    end = min(end, len(sound) / 1000)\n",
    "    trimmed_sound = sound[start * 1000 : end * 1000]\n",
    "    trimmed_sound.export(out_path, format=\"wav\")\n",
    "    return out_path, start, end\n",
    "\n",
    "def run_pyannote_vad(file):\n",
    "    vad_segments = pipeline(file)\n",
    "    pyannote_timeline = vad_segments.get_timeline().support()\n",
    "    response_timeline = []\n",
    "    for segment in pyannote_timeline:\n",
    "        start, end = list(segment)\n",
    "        response_timeline.append((start, end))\n",
    "    # get start of first and end of last\n",
    "    if len(response_timeline) > 0:\n",
    "        start = response_timeline[0][0]\n",
    "        end = response_timeline[-1][1]\n",
    "    else:\n",
    "        start = 0\n",
    "        end = 0\n",
    "    response_timeline = [(start, end)]\n",
    "    return response_timeline\n",
    "\n",
    "def run_silerio_vad(file):\n",
    "    wav = read_audio(file, sampling_rate=SAMPLING_RATE)\n",
    "    # get speech timestamps from full audio file\n",
    "    silerio_timeline = get_speech_timestamps(wav, model, sampling_rate=SAMPLING_RATE)\n",
    "    # show the timestamps as s\n",
    "    silerio_timeline = [(segment[\"start\"] / SAMPLING_RATE, segment[\"end\"] / SAMPLING_RATE) for segment in silerio_timeline]\n",
    "\n",
    "    # get start of first and end of last\n",
    "    if len(silerio_timeline) > 0:\n",
    "        start = silerio_timeline[0][0]\n",
    "        end = silerio_timeline[-1][1]\n",
    "    else:\n",
    "        start = 0\n",
    "        end = 0\n",
    "    silerio_timeline = [(start, end)]\n",
    "    return silerio_timeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files_folder = \"/data/tts-qa/tts-data/French(Dorsaf)/trimmed\"\n",
    "# files_folder = \"/data/tts-qa/tts-data/German(Dorothee)/trimmed\"\n",
    "# files_folder = \"/data/tts-qa/tts-data/English(Melynda)/trimmed\"\n",
    "# files_folder = \"/data/tts-qa/tts-data/Italian(Martina) Deliverable 3/trimmed\"\n",
    "files_folder = \"/data/tts-qa/tts-data/Spanish(Violeta) Deliverable 3/trimmed\"\n",
    "\n",
    "\n",
    "files = glob(os.path.join(files_folder, \"*.wav\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select  files\n",
    "\n",
    "np.random.seed(10)\n",
    "selected_files = np.random.choice(files, 50, replace=False)\n",
    "\n",
    "# selected_files = [\n",
    "#     \"/data/tts-qa/tts-data/French(Dorsaf)/trimmed/FR00000026.wav\",\n",
    "#     \"/data/tts-qa/tts-data/French(Dorsaf)/trimmed/FR00000032.wav\",\n",
    "#     \"/data/tts-qa/tts-data/French(Dorsaf)/trimmed/FR00000033.wav\",\n",
    "#     \"/data/tts-qa/tts-data/French(Dorsaf)/trimmed/FR00000035.wav\",\n",
    "#     \"/data/tts-qa/tts-data/French(Dorsaf)/trimmed/FR00001001.wav\",\n",
    "# ]\n",
    "\n",
    "\n",
    "# rare_cases = [\n",
    "#     \"/data/tts-qa/tts-data/French(Dorsaf)/raw/FR00000907.wav\",\n",
    "#     \"/data/tts-qa/tts-data/French(Dorsaf)/raw/FR00000115.wav\",\n",
    "#     \"/data/tts-qa/tts-data/French(Dorsaf)/raw/FR00000169.wav\",\n",
    "#     \"/data/tts-qa/tts-data/French(Dorsaf)/raw/FR00000952.wav\",\n",
    "#     \"/data/tts-qa/tts-data/French(Dorsaf)/raw/FR00000584.wav\",\n",
    "#     \"/data/tts-qa/tts-data/French(Dorsaf)/raw/FR00000911.wav\",\n",
    "#     \"/data/tts-qa/tts-data/French(Dorsaf)/raw/FR00001238.wav\",\n",
    "#     \"/data/tts-qa/tts-data/French(Dorsaf)/raw/FR00000979.wav\",\n",
    "# ]\n",
    "# selected_files= rare_cases\n",
    "\n",
    "# selected_files = [\n",
    "#     \"/data/tts-qa/tts-data/German(Dorothee)/trimmed/DE00080623.wav\",\n",
    "#     \"/data/tts-qa/tts-data/German(Dorothee)/trimmed/DE00069958.wav\",\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def my_custom_vad(pyannote_segment, silerio_segment, waveform, sample_rate=SAMPLING_RATE, energy_threshold=500000, window_size=0.02):\n",
    "    merged_timeline = []\n",
    "    # Your logic for merging or comparing pyannote and silerio timelines\n",
    "    pyannote_start, pyannote_end = list(deepcopy(pyannote_segment))\n",
    "    silerio_start, silerio_end = list(deepcopy(silerio_segment))\n",
    "\n",
    "    # If the segments are close enough, merge them\n",
    "    if abs(pyannote_start - silerio_start) < 0.05:\n",
    "        merged_start = min(pyannote_start, silerio_start)\n",
    "    else:\n",
    "        # Divide the segment into smaller windows and check energy\n",
    "        start_start = min(pyannote_start, silerio_start)\n",
    "        start_end = max(pyannote_start, silerio_start)\n",
    "        merged_timeline = []\n",
    "        merged_start = start_end\n",
    "        num_windows = int((start_end - start_start) / window_size)\n",
    "        for i in range(num_windows):\n",
    "            window_start = int((start_start + i * window_size) * sample_rate)\n",
    "            window_end = int(window_start + window_size * sample_rate)\n",
    "            window_samples = waveform[window_start:window_end]\n",
    "            window_energy = np.sum(window_samples ** 2) / len(window_samples)\n",
    "\n",
    "            if window_energy > energy_threshold:\n",
    "                merged_timeline.append((start_start + i * window_size, start_start + (i + 1) * window_size, window_energy))\n",
    "        if len(merged_timeline)>0:\n",
    "            merged_start = merged_timeline[0][0]\n",
    "        \n",
    "    if abs(pyannote_end - silerio_end) < 0.05:\n",
    "        merged_end = max(pyannote_end, silerio_end)\n",
    "    else:\n",
    "        end_start = min(pyannote_end, silerio_end)\n",
    "        end_end = max(pyannote_end, silerio_end)\n",
    "        merged_timeline = []\n",
    "        merged_end = end_start\n",
    "        num_windows = int((end_end - end_start) / window_size)\n",
    "        for i in range(num_windows):\n",
    "            window_start = int((end_start + i * window_size) * sample_rate)\n",
    "            window_end = int(window_start + window_size * sample_rate)\n",
    "            window_samples = waveform[window_start:window_end]\n",
    "            window_energy = np.sum(window_samples ** 2) / len(window_samples)\n",
    "\n",
    "            if window_energy > energy_threshold:\n",
    "                merged_timeline.append((end_start + i * window_size, end_start + (i + 1) * window_size, window_energy))\n",
    "        if len(merged_timeline)>0:\n",
    "            merged_end = merged_timeline[-1][1]\n",
    "\n",
    "    custom_segment = (merged_start, merged_end)\n",
    "    return custom_segment, merged_timeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(waveform, segment):\n",
    "    start = segment[0]\n",
    "    end=segment[1]\n",
    "    start = max(0, start-padding)\n",
    "    end = min(len(waveform) / SAMPLING_RATE, end+padding)\n",
    "    return (start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tempfile\n",
    "\n",
    "# Create a temporary directory\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "for file in selected_files:\n",
    "    # load the waveform\n",
    "    audio = AudioSegment.from_file(file, format=\"wav\")\n",
    "    waveform = np.array(audio.get_array_of_samples())\n",
    "\n",
    "\n",
    "    original_sampling_rate = audio.frame_rate\n",
    "    target_sampling_rate = SAMPLING_RATE\n",
    "\n",
    "    resampled_waveform = signal.resample(waveform, int(len(waveform) * target_sampling_rate / original_sampling_rate))\n",
    "\n",
    "    temp_wav_file = os.path.join(temp_dir, \"temp_audio.wav\")\n",
    "    wav.write(temp_wav_file, target_sampling_rate, resampled_waveform.astype('int16'))\n",
    "\n",
    "    pyannote_segment = run_pyannote_vad(temp_wav_file)[0]\n",
    "    silerio_segment = run_silerio_vad(temp_wav_file)[0]\n",
    "    pyannote_segment = pad(resampled_waveform, pyannote_segment)\n",
    "    silerio_segment = pad(resampled_waveform, silerio_segment)\n",
    "\n",
    "\n",
    "    custom_segment, merged = my_custom_vad(pyannote_segment, silerio_segment, resampled_waveform, SAMPLING_RATE)\n",
    "    custom_segment = pad(resampled_waveform, custom_segment)\n",
    "\n",
    "    # Calculate time vector\n",
    "    time_vector = np.linspace(0, len(resampled_waveform) / SAMPLING_RATE, num=len(resampled_waveform))\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 5), dpi=50)\n",
    "    plt.yticks([])\n",
    "    plt.plot(time_vector, resampled_waveform)\n",
    "    # add the filename as title\n",
    "    plt.title(os.path.basename(file))\n",
    "\n",
    "#     plt.axvspan(silerio_segment[0], silerio_segment[1] , color=\"red\", alpha=0.5, label=\"SILERIO-VAD\")\n",
    "\n",
    "\n",
    "    plt.axvspan(pyannote_segment[0], pyannote_segment[1], color=\"green\", alpha=0.3, label=\"PYANNOTE\")\n",
    "\n",
    "\n",
    "\n",
    "    plt.axvspan(custom_segment[0], custom_segment[1], color=\"black\", alpha=0.3, label=\"MY_CUSTOM_VAD\")\n",
    "\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show() \n",
    "\n",
    "# Cleanup: Delete the temporary directory and its contents\n",
    "for file_name in os.listdir(temp_dir):\n",
    "    file_path = os.path.join(temp_dir, file_name)\n",
    "    if os.path.isfile(file_path):\n",
    "        os.unlink(file_path)\n",
    "os.rmdir(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "corpus-insight",
   "language": "python",
   "name": "corpus-insight"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
