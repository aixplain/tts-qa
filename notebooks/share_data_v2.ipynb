{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to postgresql db usingenvironment variable read from vars.env \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# read environment variables from vars.env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../vars.env\")\n",
    "\n",
    "# connect to postgresql db on localhost, post 5432, using user and password from vars.env\n",
    "\n",
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# Define the database credentials\n",
    "db_host = os.getenv(\"POSTGRES_HOST\")\n",
    "db_name = os.getenv(\"POSTGRES_DB\")\n",
    "db_user = os.getenv(\"POSTGRES_USER\")\n",
    "db_password = os.getenv(\"POSTGRES_PWD\")\n",
    "\n",
    "# Create the connection\n",
    "connection = psycopg2.connect(\n",
    "    host=db_host,\n",
    "    database=db_name,\n",
    "    user=db_user,\n",
    "    password=db_password\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove folders under/data/tts-qa/share_*'that starts with share \n",
    "\n",
    "# !rm -rf /data/tts-qa/share_* # be careful with this command, it will remove all folders under /data/tts-qa/share_* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hours = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "for dataset_str in [\"German(Dorothee)\", \"French(Dorsaf)\", \"English(Melynda)\",  \"Italian(Martina)\", \"Spanish(Violeta)\"]:\n",
    "    # Define the path to the SQL scriptorder by sample.wer desc \n",
    "    query = f\"\"\"\n",
    "    SELECT sample.* \n",
    "    FROM sample\n",
    "    JOIN dataset ON sample.dataset_id = dataset.id\n",
    "    WHERE dataset.name='{dataset_str}' \n",
    "        AND sample.custom_trimmed_audio_duration > 0 \n",
    "        AND sample.custom_trimmed_audio_duration < 5 \n",
    "        AND sample.longest_pause < 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    # Assume you've loaded the sample and annotation tables as DataFrames\n",
    "    sample_df = pd.read_sql(query, connection)\n",
    "    annotation_df = pd.read_sql(\"SELECT * FROM annotation\", connection)\n",
    "\n",
    "    # Merge the DataFrames based on the sample.id and annotation.sample_id\n",
    "    merged_df = pd.merge(sample_df, annotation_df, left_on='id', right_on='sample_id', how='left')\n",
    "\n",
    "    # discard th discarded\n",
    "    merged_df = merged_df[merged_df['status'] != 'Discarded']\n",
    "\n",
    "    # drop  duplicates in id_x and keep the first one\n",
    "    merged_df.drop_duplicates(subset=['id_x'], keep='first', inplace=True)\n",
    "\n",
    "    #rename idx to id\n",
    "    merged_df.rename(columns={'id_x': 'id'}, inplace=True)\n",
    "\n",
    "\n",
    "    # Compute the required columns\n",
    "    merged_df['words_per_second'] = merged_df['sentence_length'] / merged_df['custom_trimmed_audio_duration']\n",
    "    merged_df['text'] = merged_df['final_text'].combine_first(merged_df['original_text'])\n",
    "    merged_df['wer'] = merged_df.apply(lambda row: (row['wer'] + row['uncased_unpunctuated_wer'])/2 if pd.isnull(row['final_text']) else 0, axis=1)\n",
    "    merged_df['wer_wo_punctuation'] = merged_df.apply(lambda row: row['uncased_unpunctuated_wer'] if pd.isnull(row['final_text']) else 0, axis=1)\n",
    "    merged_df['has_annotation'] = ~merged_df['final_text'].isnull()\n",
    "    merged_df['duration'] = merged_df['custom_trimmed_audio_duration']\n",
    "\n",
    "    # Select the columns you want in the final result\n",
    "    df = merged_df[['id', 'filename', 'local_custom_trimmed_path', 'words_per_second', 'text', 'wer', 'wer_wo_punctuation', 'has_annotation',  'duration']]\n",
    "\n",
    "\n",
    "    # Optionally, you can sort by the has_annotation column\n",
    "    df = df.sort_values(by='has_annotation', ascending=False)\n",
    "\n",
    "\n",
    "    # Execute the SQL script into pandas dataframe with column names\n",
    "    df.sort_values(by=['wer'], inplace=True, ascending=True)\n",
    "    df = df[df['wer'] <=0.5]\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "    from numpy import percentile\n",
    "    wps = df['words_per_second'].values\n",
    "\n",
    "    # calculate interquartile range\n",
    "    q25, q75 = percentile(wps, 25), percentile(wps, 75)\n",
    "    iqr = q75 - q25\n",
    "\n",
    "    # calculate the outlier cutoff\n",
    "    cut_off = iqr * 1.25\n",
    "    lower, upper = q25 - cut_off, q75 + cut_off\n",
    "\n",
    "    print(f\"Lower: {lower}, Upper: {upper}\")\n",
    "    # identify outliers\n",
    "    df[\"is_outlier\"] = (df['words_per_second'] < lower) | (df['words_per_second'] > upper)\n",
    "    outliers = df[df['is_outlier'] == True]\n",
    "    print('Identified outliers: %d' % len(outliers))\n",
    "\n",
    "    df = df[df['is_outlier'] == False]\n",
    "    df.drop(['is_outlier'], axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # create a language folder under it \n",
    "    if \"English\" in dataset_str:\n",
    "        dataset = \"English\"\n",
    "    elif \"Spanish\" in dataset_str:\n",
    "        dataset = \"Spanish\"\n",
    "    elif \"German\" in dataset_str:\n",
    "        dataset = \"German\"\n",
    "    elif \"French\" in dataset_str:\n",
    "        dataset = \"French\"\n",
    "    elif \"Italian\" in dataset_str:\n",
    "        dataset = \"Italian\"\n",
    "\n",
    "    df_share_all = df[df.wer_wo_punctuation ==0]\n",
    "    if df_share_all['duration'].sum() / 60 / 60 < total_hours:\n",
    "        cutoff_idx = df['duration'].cumsum().searchsorted(total_hours*60*60)\n",
    "        df_share_wavs = df.iloc[:cutoff_idx]\n",
    "        # empty df\n",
    "        df_share_extras = pd.DataFrame(columns=df.columns)\n",
    "    else:\n",
    "        cutoff_idx = df_share_all['duration'].cumsum().searchsorted(total_hours*60*60)\n",
    "        df_share_wavs = df_share_all.iloc[:cutoff_idx]\n",
    "        df_share_extras = df_share_all.iloc[cutoff_idx:]\n",
    "\n",
    "   \n",
    "\n",
    "    # make a share folder\n",
    "    share_folder = f\"/data/tts-qa/share_{total_hours}h\"\n",
    "    if not os.path.exists(share_folder):\n",
    "        os.mkdir(share_folder)\n",
    "\n",
    "\n",
    "    lang_folder = os.path.join(share_folder, dataset)\n",
    "    if not os.path.exists(lang_folder):\n",
    "        os.mkdir(lang_folder)\n",
    "\n",
    "    # create \"wav\" folder under the language folder\n",
    "    wav_folder = os.path.join(lang_folder, \"wavs\")\n",
    "    if not os.path.exists(wav_folder):\n",
    "        os.mkdir(wav_folder)\n",
    "    \n",
    "    extras_folder = os.path.join(lang_folder, \"extras\")\n",
    "    if not os.path.exists(extras_folder):\n",
    "        os.mkdir(extras_folder)\n",
    "\n",
    "    # copy audio files to the language folder\n",
    "    import shutil\n",
    "    from tqdm import tqdm\n",
    "    df_share_all_concat = []\n",
    "    for df_share, folder in [(df_share_wavs, 'wavs'), (df_share_extras, 'extras')]:\n",
    "        if folder == 'wavs':\n",
    "            to_folder = wav_folder\n",
    "        else:\n",
    "            to_folder = extras_folder\n",
    "        for index, row in  tqdm(df_share.iterrows(), total=df_share.shape[0]):\n",
    "            shutil.copy(row['local_custom_trimmed_path'], to_folder)\n",
    "        df_share[\"filepath\"] = df_share[\"filename\"].apply(lambda x: os.path.join(folder, x))\n",
    "        df_share_all_concat.append(df_share)\n",
    "    \n",
    "    df_share = pd.concat(df_share_all_concat)\n",
    "    # drop the local_custom_trimmed_path  and wer, id columns\n",
    "    df_share_clean = df_share.drop(['local_custom_trimmed_path','words_per_second', 'id'], axis=1)\n",
    "    # create a csv file with the same name as the language folder\n",
    "    # sort on by filename\n",
    "    df_share_clean = df_share_clean[['filename','filepath', 'text', 'duration', 'wer','wer_wo_punctuation']]\n",
    "    # df_share_clean.to_csv(os.path.join(lang_folder, dataset + \".csv\"), index=False)\n",
    "    # save wavs and extras separately\n",
    "    df_share_clean_wavs = df_share_clean[~df_share_clean['filepath'].str.contains(\"extras\")]\n",
    "    df_share_clean_wavs.to_csv(os.path.join(lang_folder, f\"{dataset}.csv\"), index=False)\n",
    "    df_share_clean_extras = df_share_clean[df_share_clean['filepath'].str.contains(\"extras\")]\n",
    "    df_share_clean_extras.to_csv(os.path.join(lang_folder, f\"{dataset}-extras.csv\"), index=False)\n",
    "    \n",
    "    stats[f\"{dataset}\"] = {\n",
    "        \"total_duration (hours)\": df_share_clean_wavs['duration'].sum() / 60 / 60,\n",
    "        \"file count\" : df_share_clean_wavs.shape[0],\n",
    "        \"wer=0 (percentage)\": df_share_clean_wavs[df_share_clean_wavs['wer'] == 0].shape[0] / df_share_clean_wavs.shape[0],\n",
    "        \"avg wer (wer!=0)\": df_share_clean_wavs[df_share_clean_wavs['wer'] != 0]['wer'].mean(),\n",
    "    }\n",
    "\n",
    "    if df_share_clean_extras.shape[0] > 0:\n",
    "        stats[f\"{dataset}-extras\"] = {\n",
    "            \"total_duration (hours)\": df_share_clean_extras['duration'].sum() / 60 / 60,\n",
    "            \"file count\" : df_share_clean_extras.shape[0],\n",
    "            \"wer=0 (percentage)\": df_share_clean_extras[df_share_clean_extras['wer'] == 0].shape[0] / df_share_clean_extras.shape[0],\n",
    "            \"avg wer (wer!=0)\": df_share_clean_extras[df_share_clean_extras['wer'] != 0]['wer'].mean(),\n",
    "        }\n",
    "\n",
    "    print(f\"Total duration: {df_share_clean['duration'].sum() / 60 / 60} hours\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_share_clean_wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "for dataset in [\"German\", \"French\", \"English\", \"Italian\", \"Spanish\"]:\n",
    "    # for each of the dataset generate a random 100 sample and save it to a csv file as well as the audio files\n",
    "    sample_folder = \"sample_100\"\n",
    "    if not os.path.exists(os.path.join(sample_folder, dataset)):\n",
    "        os.makedirs(os.path.join(sample_folder, dataset))\n",
    "\n",
    "    if not os.path.exists(os.path.join(sample_folder, dataset, \"wavs\")):\n",
    "        os.makedirs(os.path.join(sample_folder, dataset, \"wavs\"))\n",
    "        \n",
    "    df = pd.read_csv(f\"/data/tts-qa/share_{total_hours}h/{dataset}/{dataset}.csv\")\n",
    "\n",
    "\n",
    "    df_sample = df.sample(n=100, random_state=1)\n",
    "    df_sample.to_csv(f\"{sample_folder}/{dataset}/{dataset}.csv\", index=False)\n",
    "\n",
    "    # copy audio files to the language folder\n",
    "    for index, row in  tqdm(df_sample.iterrows(), total=df_sample.shape[0]):\n",
    "        shutil.copy(f\"/data/tts-qa/share_{total_hours}h/{dataset}/{row['filepath']}\", os.path.join(sample_folder, dataset, \"wavs\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip the sample_100 folder\n",
    "import shutil\n",
    "\n",
    "shutil.make_archive('sample_100', 'zip', 'sample_100')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['wer'], inplace=True, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.wer.plot.hist(bins=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Bash Script for Sharing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write down a bash script to go the /data/tts-qa and zip all the folders in a for loop \n",
    "# then upload the zip file to s3://user-ahmet/translated-Spanish-Italian-French-10h.zip\n",
    "\n",
    "with open(os.path.join('/data/tts-qa', \"zip.sh\"), \"w\") as f:\n",
    "    f.write(\"#!/bin/bash\\n\")\n",
    "    f.write(f\"cd /data/tts-qa/share_{total_hours}h\\n\")\n",
    "    \n",
    "    f.write(\"for d in */ ; do\\n\")\n",
    "    # add logging \n",
    "    f.write(\"    echo \\\"zipping $d\\\"\\n\")\n",
    "    # get the name \n",
    "    f.write(\"    folder=${d%/}\\n\")\n",
    "    f.write(\"    zip -r $folder.zip $d\\n\")\n",
    "    f.write(\"done\\n\")\n",
    "    # include ony zip\n",
    "    f.write(f\"aws s3 cp --recursive  --exclude \\\"*\\\" --include \\\"*.zip\\\" ./ s3://user-ahmet/Translated-{total_hours}h-v2/\\n\")\n",
    "    f.write(\"echo \\\"done\\\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loisten audio\n",
    "import IPython.display as ipd\n",
    "\n",
    "\n",
    "index = -3\n",
    "print(df_share.iloc[index][\"filename\"])\n",
    "print(df_share.iloc[index][\"text\"])\n",
    "ipd.Audio(df_share.iloc[index][\"local_custom_trimmed_path\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(df_share.iloc[index][\"local_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_share.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write a bash script to get dump of a database for every hour and upload to s3 \n",
    "# # commands looks like following\n",
    "# # docker exec -t postgres_container_dev pg_dump -U postgres  dev_tts_db > /data/tts-qa/dumps/dump_`date +%Y-%m-%d\"_\"%H_%M_%S`.sql\n",
    "# # aws s3 sync /data/tts-qa/dumps/ s3://user-ahmet/tts-qa-dumps/\n",
    "# import os\n",
    "# with open(os.path.join('..', \"dump.sh\"), \"w\") as f:\n",
    "#     f.write(\"#!/bin/bash\\n\")\n",
    "#     f.write(f\"cd /data/tts-qa/dumps\\n\")\n",
    "#     f.write(\"docker exec -t postgres_container_dev pg_dump -U postgres  dev_tts_db > /data/tts-qa/dumps/dump_`date +%Y-%m-%d\\\"_\\\"%H_%M_%S`.sql\\n\")\n",
    "#     f.write(\"aws s3 sync /data/tts-qa/dumps/ s3://user-ahmet/tts-qa-dumps/\\n\")\n",
    "#     f.write(\"echo \\\"done\\\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write cron job to run every hour\n",
    "# crontab -e\n",
    "# 0 * * * * /data/tts-qa/dump.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "corpus-insight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
