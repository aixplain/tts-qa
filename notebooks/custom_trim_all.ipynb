{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import psycopg2\n",
    "import wave\n",
    "from pydub import AudioSegment\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Assuming custom_vad_function is a function you have that takes a filename and returns new start and end trim times\n",
    "from src.utils.audio import CustomVAD, trim_audio\n",
    "my_custom_vad = CustomVAD(pyannote_model_path=\"pyannote/segmentation\", silero_model_path=\"snakers4/silero-vad\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(\"../vars.env\")\n",
    "\n",
    "# Database credentials\n",
    "db_host = os.getenv(\"POSTGRES_HOST\")\n",
    "db_name = os.getenv(\"POSTGRES_DB\")\n",
    "db_user = os.getenv(\"POSTGRES_USER\")\n",
    "db_password = os.getenv(\"POSTGRES_PWD\")\n",
    "\n",
    "# Establish a database connection\n",
    "conn = psycopg2.connect(host=db_host, database=db_name, user=db_user, password=db_password)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Retrieve all datasets\n",
    "cur.execute(\"SELECT id, name FROM dataset;\")\n",
    "datasets = cur.fetchall()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each of the folder under /data/tts-qa/tts-data/ go one level down an generate trimmed2 folder\n",
    "folders = os.listdir('/data/tts-qa/tts-data/')\n",
    "\n",
    "for folder in folders:\n",
    "    if os.path.isdir('/data/tts-qa/tts-data/' + folder):\n",
    "        if not os.path.exists('/data/tts-qa/tts-data/' + folder + '/trimmed2'):\n",
    "            os.mkdir('/data/tts-qa/tts-data/' + folder + '/trimmed2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.db_utils import evaluate_audio, convert_to_88k, convert_to_mono, normalize_audio, convert_to_s16le\n",
    "# Process each dataset\n",
    "for dataset_id, dataset_name in datasets:\n",
    "    if dataset_name != 'German(Dorothee)':\n",
    "        continue\n",
    "    print(f\"Processing dataset: {dataset_name}\")\n",
    "\n",
    "    # Retrieve all samples that are selected for delivery from the current dataset\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT id, filename, local_path, local_trimmed_path\n",
    "        FROM sample\n",
    "        WHERE is_selected_for_delivery = TRUE AND\n",
    "                dataset_id = %s AND  \n",
    "                local_custom_trimmed_path IS NULL;\n",
    "    \"\"\", (dataset_id,))\n",
    "    samples = cur.fetchall()\n",
    "\n",
    "    # Loop over each sample\n",
    "    for id, filename, local_path, local_trimmed_path in tqdm(samples):\n",
    "        # Define the new trimmed path  from '/data/tts-qa/tts-data/French(Dorsaf) Deliverable 7/trimmed/FR00054280.wav'\n",
    "        trimmed2_path = local_trimmed_path.replace('trimmed', 'trimmed2')\n",
    "        # Run custom VAD to get new trim times\n",
    "        response = my_custom_vad.process_file(local_path)\n",
    "\n",
    "        trim_start, trim_end = tuple(response['custom_segment'])\n",
    "\n",
    "        # round 2 \n",
    "        trim_start = round(trim_start, 2)\n",
    "        trim_end = round(trim_end, 2)\n",
    "\n",
    "        # Load the audio file\n",
    "        audio = AudioSegment.from_wav(local_path)\n",
    "\n",
    "        \n",
    "        # Trim the audio\n",
    "        trim_audio(local_path, trim_start, trim_end, trimmed2_path)\n",
    "        meta = evaluate_audio(trimmed2_path)\n",
    "\n",
    "        if meta[\"is_88khz\"] == False:\n",
    "            convert_to_88k(local_path, local_path)\n",
    "        \n",
    "        if meta[\"is_mono\"] == False:\n",
    "            convert_to_mono(local_path, local_path)\n",
    "\n",
    "        if meta[\"peak_volume_db\"] < -6 or meta[\"peak_volume_db\"] > -3:\n",
    "            normalize_audio(local_path, local_path)\n",
    "\n",
    "        if meta[\"isPCM\"] == False:\n",
    "            convert_to_s16le(local_path, local_path)\n",
    "\n",
    "        try:\n",
    "            # Update the database with the new trim times (if necessary)\n",
    "            cur.execute(\"\"\"\n",
    "                UPDATE sample\n",
    "                SET trim_custom_start = %s, trim_custom_end = %s, local_custom_trimmed_path = %s\n",
    "                WHERE id = %s;\n",
    "            \"\"\", (trim_start, trim_end, trimmed2_path, id))\n",
    "            conn.commit()\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating database: {e}\")\n",
    "            os.remove(trimmed2_path)\n",
    "            conn.rollback()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the database connection\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define the path to the zip file\n",
    "zip_file_path = \"random_samples.zip\"\n",
    "\n",
    "# Open the zip file in write mode\n",
    "with zipfile.ZipFile(zip_file_path, mode=\"w\") as zip_file:\n",
    "    # Loop over each dataset\n",
    "    for dataset_id, dataset_name in datasets:\n",
    "        print(f\"Processing dataset: {dataset_name}\")\n",
    "\n",
    "        # Retrieve all samples from the current dataset\n",
    "        samples = pd.read_sql(f\"\"\"\n",
    "            SELECT id, filename, local_custom_trimmed_path\n",
    "            FROM sample\n",
    "            WHERE dataset_id = {dataset_id} and local_custom_trimmed_path is not NULL;\n",
    "        \"\"\", conn)\n",
    "        \n",
    "        if len(samples) < 100:\n",
    "            continue\n",
    "        # skip the sampls where local_custom_trimmed_path does not exist check by os.path.exists\n",
    "\n",
    "        samples = samples[samples['local_custom_trimmed_path'].apply(lambda x: os.path.exists(x))]\n",
    "        # Select random 100 samples\n",
    "        random_samples = samples.sample(n=100, random_state=42)\n",
    "\n",
    "        # Loop over each random sample\n",
    "        for id, filename, local_custom_trimmed_path in tqdm(random_samples.values):\n",
    "            # Add the sample to the zip file\n",
    "            zip_file.write(local_custom_trimmed_path, arcname=f\"{dataset_name}/{filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "corpus-insight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
