{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to postgresql db usingenvironment variable read from vars.env \n",
    "\n",
    "import os\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# read environment variables from vars.env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../vars.env\")\n",
    "\n",
    "# connect to postgresql db on localhost, post 5432, using user and password from vars.env\n",
    "\n",
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# Define the database credentials\n",
    "db_host = os.getenv(\"POSTGRES_HOST\")\n",
    "db_name = os.getenv(\"POSTGRES_DB\")\n",
    "db_user = os.getenv(\"POSTGRES_USER\")\n",
    "db_password = os.getenv(\"POSTGRES_PWD\")\n",
    "total_hours = 30\n",
    "\n",
    "for dataset in [\"French\"]:\n",
    "    print(f\"Processing {dataset} dataset\")\n",
    "    # Define the path to the SQL scriptorder by sample.wer desc \n",
    "    sql_script = f\"\"\"\n",
    "    SELECT sample.id, sample.filename, sample.local_trimmed_path, sample.original_text as text, sample.wer, sample.trimmed_audio_duration as duration\n",
    "    FROM sample\n",
    "    JOIN dataset ON sample.dataset_id = dataset.id\n",
    "    WHERE dataset.name LIKE '%' || '{dataset}' || '%';\n",
    "    \"\"\"\n",
    "\n",
    "    # Connect to the database\n",
    "    conn = psycopg2.connect(\n",
    "        host=db_host,\n",
    "        database=db_name,\n",
    "        user=db_user,\n",
    "        password=db_password\n",
    "    )\n",
    "\n",
    "    # Execute the SQL script into pandas dataframe with column names\n",
    "    df = pd.read_sql_query(sql_script, conn)\n",
    "    df.sort_values(by=['wer'], inplace=True, ascending=True)\n",
    "\n",
    "    # get 10 hours of audio with lowest wer\n",
    "    # find index that the sum of duration is 10 hours\n",
    "\n",
    "    cutoff_idx = df['duration'].cumsum().searchsorted(total_hours*60*60)\n",
    "    df_share = df.iloc[:cutoff_idx]\n",
    "\n",
    "    # make a share fodler\n",
    "    share_folder = f\"/data/tts-qa/share_{total_hours}h\"\n",
    "    if not os.path.exists(share_folder):\n",
    "        os.mkdir(share_folder)\n",
    "\n",
    "    # create a language folder under it \n",
    "    lang_folder = os.path.join(share_folder, dataset)\n",
    "    if not os.path.exists(lang_folder):\n",
    "        os.mkdir(lang_folder)\n",
    "\n",
    "    # create \"wav\" folder under the language folder\n",
    "    wav_folder = os.path.join(lang_folder, \"wavs\")\n",
    "    if not os.path.exists(wav_folder):\n",
    "        os.mkdir(wav_folder)\n",
    "\n",
    "    # copy audio files to the language folder\n",
    "    import shutil\n",
    "    from tqdm import tqdm\n",
    "    for index, row in  tqdm(df_share.iterrows(), total=df_share.shape[0]):\n",
    "        shutil.copy(row['local_trimmed_path'], wav_folder)\n",
    "\n",
    "    # drop the local_trimmed_path  and wer, id columns\n",
    "    df_share_clean = df_share.drop(['local_trimmed_path', 'wer', 'id'], axis=1)\n",
    "    # create a csv file with the same name as the language folder\n",
    "    # sort on by filename\n",
    "    df_share_clean.sort_values(by=['filename'], inplace=True, ascending=True)\n",
    "    df_share_clean.to_csv(os.path.join(lang_folder, dataset + \".csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write down a bash script to go the /data/tts-qa and zip all the folders in a for loop \n",
    "# then upload the zip file to s3://user-ahmet/translated-Spanish-Italian-French-10h.zip\n",
    "\n",
    "with open(os.path.join('/data/tts-qa', \"zip.sh\"), \"w\") as f:\n",
    "    f.write(\"#!/bin/bash\\n\")\n",
    "    f.write(f\"cd /data/tts-qa/share_{total_hours}h\\n\")\n",
    "    \n",
    "    f.write(\"for d in */ ; do\\n\")\n",
    "    # add logging \n",
    "    f.write(\"    echo \\\"zipping $d\\\"\\n\")\n",
    "    # get the name \n",
    "    f.write(\"    folder=${d%/}\\n\")\n",
    "    f.write(\"    zip -r $folder.zip $d\\n\")\n",
    "    f.write(\"done\\n\")\n",
    "    # include ony zip\n",
    "    f.write(f\"aws s3 cp --recursive  --exclude \\\"*\\\" --include \\\"*.zip\\\" ./ s3://user-ahmet/Spanish-Italian-{total_hours}h/\\n\")\n",
    "    f.write(\"echo \\\"done\\\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_en = pd.read_csv(\"/data/tts-qa/share_30h/Spanish/Spanish.csv\")\n",
    "df_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_share_clean.duration.sum() / 60 / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_share.sort_values(by=['wer'], inplace=True, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_share.wer.plot.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "corpus-insight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
